#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ±½è½¦æ•°æ®çˆ¬è™«ç³»ç»Ÿ - è½¦168ä¸“ç‰ˆ
ä»è½¦168ç½‘ç«™çˆ¬å–çœŸå®æ±½è½¦æ•°æ®ï¼Œç›®æ ‡20000+æ¡æ•°æ®
ä¸“æ³¨äºè½¦168ä¸€ä¸ªæ•°æ®æºï¼Œç¡®ä¿æ•°æ®è´¨é‡å’Œç¨³å®šæ€§
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import logging
from fake_useragent import UserAgent
import json
import re
from urllib.parse import urljoin, urlparse, quote
import os
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import threading
from tqdm import tqdm
from datetime import datetime
import pickle

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CarDataScraper:
    """æ±½è½¦æ•°æ®çˆ¬è™«ç±» - è½¦168ä¸“ç‰ˆ"""
    
    def __init__(self):
        self.ua = UserAgent()
        self.session = requests.Session()
        self.data = []
        self.driver = None
        self.scraped_urls = set()  # é˜²é‡å¤çˆ¬å–
        
        # åŠ¨æ€å»¶è¿Ÿé…ç½®
        self.request_delay = (0.5, 2.0)
        self.page_delay = (1.0, 3.0)
        
        # è®¾ç½®é€šç”¨è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
        }
        self.session.headers.update(self.headers)
        
        # ä»£ç†æ± 
        self.proxies = []
        self.proxy_index = 0
        self.load_proxies_from_file()
        
        # æ£€æŸ¥ç‚¹æ–‡ä»¶
        self.checkpoint_file = 'scraper_checkpoint.pkl'
        
    def load_checkpoint(self):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        try:
            if os.path.exists(self.checkpoint_file):
                with open(self.checkpoint_file, 'rb') as f:
                    checkpoint = pickle.load(f)
                    self.data = checkpoint.get('data', [])
                    self.scraped_urls = checkpoint.get('scraped_urls', set())
                    logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤: {len(self.data)} æ¡æ•°æ®, {len(self.scraped_urls)} ä¸ªå·²çˆ¬å–URL")
                    return True
        except Exception as e:
            logger.warning(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        return False
    
    def save_checkpoint(self):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        try:
            checkpoint = {
                'data': self.data,
                'scraped_urls': self.scraped_urls,
                'timestamp': datetime.now()
            }
            with open(self.checkpoint_file, 'wb') as f:
                pickle.dump(checkpoint, f)
            logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {len(self.data)} æ¡æ•°æ®")
        except Exception as e:
            logger.warning(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def setup_driver(self):
        """è®¾ç½®Selenium WebDriver"""
        if self.driver is None:
            try:
                options = Options()
                options.add_argument('--headless=new')
                options.add_argument('--no-sandbox')
                options.add_argument('--disable-dev-shm-usage')
                options.add_argument('--disable-gpu')
                options.add_argument('--disable-blink-features=AutomationControlled')
                options.add_argument('--window-size=1920,1080')
                options.add_argument('--disable-images')
                options.add_argument('--disable-extensions')
                options.add_argument('--disable-popup-blocking')
                options.add_argument(f'--user-agent={self.ua.random}')
                options.add_experimental_option('useAutomationExtension', False)
                options.add_experimental_option('excludeSwitches', ['enable-automation'])

                self.driver = webdriver.Chrome(options=options)
                self.driver.set_page_load_timeout(30)
                self.driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
                    "source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
                })
                logger.info("WebDriver åˆå§‹åŒ–æˆåŠŸ")
                return True
            except Exception as e:
                logger.error(f"WebDriver åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                return False
        return True
    
    def close_driver(self):
        """å…³é—­WebDriver"""
        if self.driver:
            try:
                self.driver.quit()
                self.driver = None
                logger.info("WebDriver å·²å…³é—­")
            except Exception as e:
                logger.warning(f"å…³é—­WebDriveræ—¶å‡ºé”™: {str(e)}")
        
    def get_random_delay(self, min_delay=None, max_delay=None):
        """è·å–éšæœºå»¶è¿Ÿæ—¶é—´"""
        if min_delay is None or max_delay is None:
            min_delay, max_delay = self.request_delay
        return random.uniform(min_delay, max_delay)
        
    def safe_request(self, url, max_retries=3):
        """å®‰å…¨çš„ç½‘ç»œè¯·æ±‚"""
        for attempt in range(max_retries):
            try:
                # éšæœºå»¶è¿Ÿ
                delay = self.get_random_delay()
                time.sleep(delay)
                
                # æ›´æ–°User-Agent
                self.session.headers['User-Agent'] = self.ua.random

                # è½®æ¢ä»£ç†
                proxies = self._get_next_proxy()

                response = self.session.get(url, timeout=15, proxies=proxies)
                response.raise_for_status()
                
                # è‡ªåŠ¨æ£€æµ‹ç¼–ç 
                if response.encoding == 'ISO-8859-1':
                    response.encoding = response.apparent_encoding
                elif not response.encoding:
                    response.encoding = 'utf-8'
                    
                return response
                
            except Exception as e:
                logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {url} - {str(e)}")
                if attempt == max_retries - 1:
                    return None
                time.sleep(self.get_random_delay(2, 5) * (attempt + 1))
        
        return None
    
    def load_proxies_from_file(self, filename='proxies.txt'):
        """ä»æ–‡ä»¶åŠ è½½ä»£ç†åˆ—è¡¨"""
        try:
            if os.path.exists(filename):
                with open(filename, 'r', encoding='utf-8') as f:
                    for line in f:
                        proxy = line.strip()
                        if proxy and not proxy.startswith('#'):
                            if '://' not in proxy:
                                proxy = f'http://{proxy}'
                            self.proxies.append({'http': proxy, 'https': proxy})
                logger.info(f"åŠ è½½äº† {len(self.proxies)} ä¸ªä»£ç†")
            else:
                logger.info("ä»£ç†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç›´è¿")
        except Exception as e:
            logger.warning(f"åŠ è½½ä»£ç†æ–‡ä»¶å¤±è´¥: {e}")
    
    def _get_next_proxy(self):
        """è·å–ä¸‹ä¸€ä¸ªä»£ç†"""
        if not self.proxies:
            return None
        
        proxy = self.proxies[self.proxy_index]
        self.proxy_index = (self.proxy_index + 1) % len(self.proxies)
        return proxy
    
    def _city_name_map(self, city_code):
        """åŸå¸‚ä»£ç è½¬ä¸­æ–‡å"""
        city_map = {
            'beijing': 'åŒ—äº¬', 'bj': 'åŒ—äº¬',
            'shanghai': 'ä¸Šæµ·', 'sh': 'ä¸Šæµ·',
            'guangzhou': 'å¹¿å·', 'gz': 'å¹¿å·',
            'shenzhen': 'æ·±åœ³', 'sz': 'æ·±åœ³',
            'hangzhou': 'æ­å·', 'hz': 'æ­å·',
            'nanjing': 'å—äº¬', 'nj': 'å—äº¬',
            'wuhan': 'æ­¦æ±‰', 'wh': 'æ­¦æ±‰',
            'chengdu': 'æˆéƒ½', 'cd': 'æˆéƒ½',
            'xian': 'è¥¿å®‰', 'xa': 'è¥¿å®‰',
            'chongqing': 'é‡åº†', 'cq': 'é‡åº†',
            'tianjin': 'å¤©æ´¥', 'tj': 'å¤©æ´¥',
            'qingdao': 'é’å²›', 'qd': 'é’å²›',
            'dalian': 'å¤§è¿', 'dl': 'å¤§è¿',
            'suzhou': 'è‹å·', 'su': 'è‹å·',
            'dongguan': 'ä¸œè', 'dg': 'ä¸œè',
            'foshan': 'ä½›å±±', 'fs': 'ä½›å±±',
            'zhengzhou': 'éƒ‘å·', 'zz': 'éƒ‘å·',
            'changsha': 'é•¿æ²™', 'cs': 'é•¿æ²™',
            'jinan': 'æµå—', 'jn': 'æµå—',
            'hefei': 'åˆè‚¥', 'hf': 'åˆè‚¥',
            'shenyang': 'æ²ˆé˜³', 'sy': 'æ²ˆé˜³',
            'changchun': 'é•¿æ˜¥', 'cc': 'é•¿æ˜¥',
            'harbin': 'å“ˆå°”æ»¨', 'hrb': 'å“ˆå°”æ»¨',
            'taiyuan': 'å¤ªåŸ', 'ty': 'å¤ªåŸ',
            'taizhou': 'å°å·', 'tz': 'å°å·',
            'ningbo': 'å®æ³¢', 'nb': 'å®æ³¢',
            'wuxi': 'æ— é”¡', 'wx': 'æ— é”¡',
            'changzhou': 'å¸¸å·',
            'xuzhou': 'å¾å·',
            'yantai': 'çƒŸå°',
            'weifang': 'æ½åŠ',
            'linyi': 'ä¸´æ²‚',
            'zibo': 'æ·„åš',
            'weihai': 'å¨æµ·',
            'dongying': 'ä¸œè¥',
            'binzhou': 'æ»¨å·',
            'dezhou': 'å¾·å·',
            'liaocheng': 'èŠåŸ',
            'heze': 'èæ³½',
            'zaozhuang': 'æ£åº„',
            'jining': 'æµå®',
            'taian': 'æ³°å®‰',
            'rizhao': 'æ—¥ç…§',
            'laiwu': 'è±èŠœ',
            'huaian': 'æ·®å®‰',
            'yancheng': 'ç›åŸ',
            'yangzhou': 'æ‰¬å·',
            'zhenjiang': 'é•‡æ±Ÿ',
            'suqian': 'å®¿è¿',
            'lianyungang': 'è¿äº‘æ¸¯',
            'shaoxing': 'ç»å…´',
            'jiaxing': 'å˜‰å…´',
            'huzhou': 'æ¹–å·',
            'lishui': 'ä¸½æ°´',
            'quzhou': 'è¡¢å·',
            'zhoushan': 'èˆŸå±±',
            'taizhou_zj': 'å°å·',
            'wenzhou': 'æ¸©å·',
            'kunming': 'æ˜†æ˜', 'km': 'æ˜†æ˜',
            'lanzhou': 'å…°å·', 'lz': 'å…°å·',
            'urumqi': 'ä¹Œé²æœ¨é½',
            'guiyang': 'è´µé˜³', 'gy': 'è´µé˜³',
            'nanning': 'å—å®', 'nn': 'å—å®',
            'haikou': 'æµ·å£', 'hk': 'æµ·å£',
            'shijiazhuang': 'çŸ³å®¶åº„', 'sjz': 'çŸ³å®¶åº„',
            'taiyuan': 'å¤ªåŸ',
            'hohhot': 'å‘¼å’Œæµ©ç‰¹',
            'changchun': 'é•¿æ˜¥'
        }
        return city_map.get(city_code, city_code)
    
    def _extract_number(self, text, pattern, default=None):
        """æå–æ•°å­—"""
        try:
            match = re.search(pattern, text)
            if match:
                return float(match.group(1))
            return default
        except:
            return default
    
    def _extract_year(self, text):
        """æå–å¹´ä»½"""
        patterns = [r'(\d{4})æ¬¾', r'(\d{4})å¹´', r'(\d{4})-']
        for pattern in patterns:
            year = self._extract_number(text, pattern)
            if year and 2000 <= year <= 2025:
                return int(year)
        return None
    
    def _extract_mileage(self, text):
        """æå–é‡Œç¨‹"""
        # ä¸‡å…¬é‡Œ
        mileage = self._extract_number(text, r'([\d.]+)ä¸‡å…¬é‡Œ')
        if mileage is not None:
            return int(mileage * 10000)
        
        # å…¬é‡Œ
        mileage = self._extract_number(text, r'([\d.]+)å…¬é‡Œ')
        if mileage is not None:
            return int(mileage)
        
        return None
    
    def _extract_displacement(self, text):
        """æå–æ’é‡"""
        displacement = self._extract_number(text, r'([\d.]+)[LT]')
        if displacement and 0.5 <= displacement <= 8.0:
            return displacement
        return None
    
    def _extract_transmission(self, text):
        """æå–å˜é€Ÿå™¨ç±»å‹"""
        if 'è‡ªåŠ¨' in text or 'AT' in text:
            return 'è‡ªåŠ¨'
        if 'æ‰‹åŠ¨' in text or 'MT' in text:
            return 'æ‰‹åŠ¨'
        if 'CVT' in text:
            return 'CVT'
        if 'åŒç¦»åˆ' in text or 'DCT' in text:
            return 'åŒç¦»åˆ'
        return 'è‡ªåŠ¨'
    
    def _parse_car_title(self, title):
        """è§£æè½¦è¾†æ ‡é¢˜è·å–å“ç‰Œå’Œè½¦å‹"""
        brands = [
            'å¥¥è¿ª', 'å®é©¬', 'å¥”é©°', 'å¤§ä¼—', 'ä¸°ç”°', 'æœ¬ç”°', 'æ—¥äº§', 'é©¬è‡ªè¾¾', 'ç°ä»£', 'èµ·äºš', 
            'ç¦ç‰¹', 'é›ªä½›å…°', 'åˆ«å…‹', 'å‡¯è¿ªæ‹‰å…‹', 'æ²ƒå°”æ²ƒ', 'æ·è±¹', 'è·¯è™', 'ä¿æ—¶æ·', 'ç‰¹æ–¯æ‹‰',
            'æ¯”äºšè¿ª', 'å‰åˆ©', 'é•¿åŸ', 'å¥‡ç‘', 'é•¿å®‰', 'è£å¨', 'åçˆµ', 'ä¼ ç¥º', 'çº¢æ——', 'é¢†å…‹',
            'è”šæ¥', 'å°é¹', 'ç†æƒ³', 'å¨é©¬', 'é›¶è·‘', 'å“ªå’', 'ææ°ª', 'å²šå›¾', 'é«˜åˆ', 'æ™ºå·±',
            'é›·å…‹è¨æ–¯', 'è‹±è²å°¼è¿ª', 'è®´æ­Œ', 'æ—è‚¯', 'å‡¯è¿ªæ‹‰å…‹', 'å…‹è±æ–¯å‹’', 'Jeep', 'é“å¥‡',
            'è²äºšç‰¹', 'é˜¿å°”æ³•ç½—å¯†æ¬§', 'ç›èæ‹‰è’‚', 'æ³•æ‹‰åˆ©', 'å…°åšåŸºå°¼', 'å®¾åˆ©', 'åŠ³æ–¯è±æ–¯',
            'é˜¿æ–¯é¡¿é©¬ä¸', 'è¿ˆå‡¯ä¼¦', 'å¸ƒåŠ è¿ª', 'å¸•åŠ å°¼', 'æŸ¯å°¼å¡æ ¼', 'äº”è±', 'å®éª', 'ä¸œé£',
            'ä¸€æ±½', 'åŒ—æ±½', 'æ±Ÿæ·®', 'æµ·é©¬', 'ä¼—æ³°', 'åŠ›å¸†', 'è§‚è‡´', 'å¯è¾°', 'æ€é“­', 'ç†å¿µ'
        ]
        
        for brand in brands:
            if brand in title:
                return brand, title
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å“ç‰Œï¼Œå°è¯•ä»æ ‡é¢˜å¼€å¤´æå–
        parts = title.split()
        if parts:
            return parts[0], title
        
        return 'æœªçŸ¥å“ç‰Œ', title
    
    def _parse_price(self, price_text):
        """è§£æä»·æ ¼æ–‡æœ¬"""
        try:
            price_match = re.search(r'([\d.]+)', price_text.replace(',', ''))
            if price_match:
                return float(price_match.group(1))
            return None
        except:
            return None
    
    def _infer_fuel_type(self, brand, model):
        """æ ¹æ®å“ç‰Œå’Œè½¦å‹æ¨æ–­ç‡ƒæ–™ç±»å‹"""
        if brand in ['ç‰¹æ–¯æ‹‰', 'è”šæ¥', 'å°é¹', 'ç†æƒ³', 'å¨é©¬', 'é›¶è·‘', 'å“ªå’']:
            return 'ç”µåŠ¨'
        elif 'EV' in model or 'ç”µ' in model:
            return 'ç”µåŠ¨'
        elif 'PHEV' in model or 'DM' in model or 'æ’ç”µ' in model:
            return 'æ’ç”µæ··åŠ¨'
        elif 'HEV' in model or 'æ··åŠ¨' in model:
            return 'æ··åŠ¨'
        else:
            return 'æ±½æ²¹'
    
    def _infer_car_type(self, model):
        """æ ¹æ®è½¦å‹æ¨æ–­è½¦è¾†ç±»å‹"""
        suv_keywords = ['SUV', 'X', 'Q', 'GL', 'GLE', 'RX', 'NX', 'CX', 'CR', 'RAV', 'XC', 'QX']
        mpv_keywords = ['MPV', 'GL8', 'ODYSSEY', 'SIENNA', 'ALPHARD', 'ELYSION']
        
        model_upper = model.upper()
        
        if any(keyword in model_upper for keyword in suv_keywords):
            return 'SUV'
        elif any(keyword in model_upper for keyword in mpv_keywords):
            return 'MPV'
        else:
            return 'è½¿è½¦'
    
    def _extract_color(self, text):
        """æå–è½¦è¾†é¢œè‰²"""
        colors = ['ç™½è‰²', 'é»‘è‰²', 'é“¶è‰²', 'ç°è‰²', 'çº¢è‰²', 'è“è‰²', 'é‡‘è‰²', 'æ£•è‰²', 'ç»¿è‰²', 'é»„è‰²', 'æ©™è‰²', 'ç´«è‰²']
        for color in colors:
            if color in text:
                return color
        return None
    
    def _extract_condition_score(self, text):
        """æå–è½¦å†µè¯„åˆ†"""
        score_match = re.search(r'è½¦å†µ[ï¼š:]?([\d.]+)åˆ†', text)
        if score_match:
            return float(score_match.group(1))
        
        # å°è¯•å…¶ä»–æ¨¡å¼
        score_match = re.search(r'è¯„åˆ†[ï¼š:]?([\d.]+)', text)
        if score_match:
            return float(score_match.group(1))
        
        # ä¸ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼Œè¿”å›None
        return None
    
    def _extract_fuel_consumption(self, text):
        """æå–æ²¹è€—"""
        consumption_match = re.search(r'æ²¹è€—[ï¼š:]?([\d.]+)L', text)
        if consumption_match:
            return float(consumption_match.group(1))
        
        consumption_match = re.search(r'([\d.]+)L/100km', text)
        if consumption_match:
            return float(consumption_match.group(1))
        
        return None
    
    def _extract_max_speed(self, text):
        """æå–æœ€é«˜æ—¶é€Ÿ"""
        speed_match = re.search(r'æœ€é«˜æ—¶é€Ÿ[ï¼š:]?([\d.]+)', text)
        if speed_match:
            return float(speed_match.group(1))
        
        speed_match = re.search(r'([\d.]+)km/h', text)
        if speed_match:
            return float(speed_match.group(1))
        
        # ä¸ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼Œè¿”å›None
        return None
    
    def _extract_acceleration(self, text):
        """æå–åŠ é€Ÿæ—¶é—´"""
        acc_match = re.search(r'åŠ é€Ÿ[ï¼š:]?([\d.]+)ç§’', text)
        if acc_match:
            return float(acc_match.group(1))
        
        acc_match = re.search(r'0-100km/h[ï¼š:]?([\d.]+)s', text)
        if acc_match:
            return float(acc_match.group(1))
        
        # ä¸ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼Œè¿”å›None
        return None
    
    def _get_price_range(self, price):
        """è·å–ä»·æ ¼åŒºé—´"""
        if price < 10:
            return '10ä¸‡ä»¥ä¸‹'
        elif price < 20:
            return '10-20ä¸‡'
        elif price < 30:
            return '20-30ä¸‡'
        elif price < 50:
            return '30-50ä¸‡'
        elif price < 100:
            return '50-100ä¸‡'
        else:
            return '100ä¸‡ä»¥ä¸Š'
    
    def _get_mileage_range(self, mileage):
        """è·å–é‡Œç¨‹åŒºé—´"""
        if mileage < 10000:
            return '1ä¸‡å…¬é‡Œä»¥ä¸‹'
        elif mileage < 30000:
            return '1-3ä¸‡å…¬é‡Œ'
        elif mileage < 50000:
            return '3-5ä¸‡å…¬é‡Œ'
        elif mileage < 100000:
            return '5-10ä¸‡å…¬é‡Œ'
        else:
            return '10ä¸‡å…¬é‡Œä»¥ä¸Š'
    
    def scrape_che168_enhanced(self, cities=None, max_pages_per_city=50, target_count=20000):
        """å¢å¼ºç‰ˆè½¦168çˆ¬è™« - å¤šåŸå¸‚å…¨è¦†ç›–"""
        logger.info("å¼€å§‹å¢å¼ºç‰ˆè½¦168æ•°æ®çˆ¬å–...")
        
        if cities is None:
            # æ‰©å±•åŸå¸‚åˆ—è¡¨ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®æºè¾¾åˆ°20000+ç›®æ ‡
            cities = [
                # ä¸€çº¿åŸå¸‚
                "bj", "sh", "gz", "sz",
                # æ–°ä¸€çº¿åŸå¸‚  
                "hz", "nj", "wh", "cd", "xa", "cq", "tj", "qd", "dl", "su",
                # äºŒçº¿åŸå¸‚
                "dg", "fs", "zz", "cs", "jn", "hf", "sy", "cc", "hrb", "nb", "wx", "tz",
                # è¡¥å……åŸå¸‚ï¼ˆç¡®ä¿è¶³å¤Ÿæ•°æ®æºï¼‰
                "km", "lz", "urumqi", "gy", "nn", "hk", "sjz", "taiyuan", "hohhot", "changchun"
            ]
        
        total_scraped = 0
        
        with tqdm(total=target_count, desc="çˆ¬å–è¿›åº¦") as pbar:
            # åŠ¨æ€è°ƒæ•´æ¯ä¸ªåŸå¸‚çš„ç›®æ ‡æ•°é‡
            per_city_min = 200  # æ¯ä¸ªåŸå¸‚æœ€å°‘çˆ¬å–200æ¡
            per_city_max = 1500  # æ¯ä¸ªåŸå¸‚æœ€å¤šçˆ¬å–1500æ¡
            per_city_target = max(per_city_min, min(per_city_max, target_count // len(cities)))
            
            # å¦‚æœç›®æ ‡æ•°é‡å¾ˆå¤§ï¼Œé€‚å½“å¢åŠ æ¯ä¸ªåŸå¸‚çš„ç›®æ ‡
            if target_count > 15000:
                per_city_target = max(per_city_target, target_count // (len(cities) - 5))  # é¢„ç•™5ä¸ªåŸå¸‚çš„ç¼“å†²
            
            logger.info(f"ğŸ“‹ åˆ†é…ç­–ç•¥: æ¯åŸå¸‚ç›®æ ‡ {per_city_target} æ¡ (æœ€å°‘{per_city_min}, æœ€å¤š{per_city_max})")
            
            for city_idx, city in enumerate(cities):
                if total_scraped >= target_count:
                    break
                
                # åŠ¨æ€è°ƒæ•´å‰©ä½™åŸå¸‚çš„ç›®æ ‡
                remaining_cities = len(cities) - city_idx
                remaining_target = target_count - total_scraped
                adjusted_target = min(per_city_max, max(per_city_min, remaining_target // max(1, remaining_cities - 1)))
                
                logger.info(f"ğŸ™ï¸ å¼€å§‹çˆ¬å–åŸå¸‚ ({city_idx+1}/{len(cities)}): {self._city_name_map(city)} (ç›®æ ‡: {adjusted_target}æ¡, å‰©ä½™: {remaining_target})")
                city_scraped = 0
                
                # å¤šä¸ªåˆ†ç±»é¡µé¢ï¼Œç¡®ä¿è¦†ç›–æ›´å…¨é¢
                categories = ['', 'suv/', 'mpv/', 'pickup/', 'coupe/']
                
                for category in categories:
                    # ä¿®æ”¹æ¡ä»¶ï¼šç¡®ä¿æ¯ä¸ªåŸå¸‚éƒ½èƒ½çˆ¬åˆ°æ•°æ®
                    if total_scraped >= target_count:
                        break
                    if city_scraped >= adjusted_target and city_scraped >= per_city_min:
                        break
                        
                    for page in range(1, max_pages_per_city + 1):
                        if total_scraped >= target_count:
                            break
                        if city_scraped >= adjusted_target and city_scraped >= per_city_min:
                            break
                            
                        try:
                            url = f"https://www.che168.com/{city}/list/{category}?page={page}"
                            
                            if url in self.scraped_urls:
                                continue
                                
                            response = self.safe_request(url)
                            if not response:
                                continue
                            
                            soup = BeautifulSoup(response.text, 'lxml')
                            
                            # å¤šç§é€‰æ‹©å™¨å°è¯•
                            selectors = [
                                "li.cards-li",
                                ".list-item",
                                ".car-item",
                                ".item-info"
                            ]
                            
                            cards = []
                            for selector in selectors:
                                cards = soup.select(selector)
                                if cards:
                                    break
                            
                            if not cards:
                                logger.debug(f"é¡µé¢æ— æ•°æ®: {url}")
                                break
                            
                            new_records = 0
                            for card in cards:
                                if total_scraped >= target_count:
                                    break
                                    
                                car_data = self._parse_che168_card_enhanced(card, city)
                                if car_data and self._is_valid_data(car_data):
                                    self.data.append(car_data)
                                    new_records += 1
                                    total_scraped += 1
                                    city_scraped += 1
                                    pbar.update(1)
                            
                            self.scraped_urls.add(url)
                            
                            if new_records > 0:
                                logger.info(f"âœ… {self._city_name_map(city)} {category} ç¬¬{page}é¡µ: +{new_records}æ¡")
                            
                            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                            if len(self.data) % 1000 == 0:
                                self.save_checkpoint()
                            
                            # å¦‚æœè¿ç»­å‡ é¡µæ²¡æ•°æ®ï¼Œè·³å‡ºåˆ†ç±»
                            if new_records == 0:
                                break
                                
                            time.sleep(self.get_random_delay(*self.page_delay))
                            
                        except Exception as e:
                            logger.error(f"çˆ¬å–é¡µé¢å‡ºé”™ {url}: {str(e)}")
                            continue
                
                logger.info(f"ğŸ {self._city_name_map(city)} å®Œæˆï¼Œè·å¾— {city_scraped} æ¡æ•°æ®")
                
                # å¦‚æœè¿™ä¸ªåŸå¸‚æ²¡æœ‰è·å–åˆ°è¶³å¤Ÿæ•°æ®ï¼Œè®°å½•è­¦å‘Š
                if city_scraped < per_city_min:
                    logger.warning(f"âš ï¸ {self._city_name_map(city)} æ•°æ®é‡ä¸è¶³: {city_scraped} < {per_city_min}")
                elif city_scraped >= adjusted_target:
                    logger.info(f"âœ… {self._city_name_map(city)} è¾¾åˆ°ç›®æ ‡: {city_scraped} >= {adjusted_target}")
                
                # åŸå¸‚é—´ä¼‘æ¯
                if city_scraped > 0:
                    time.sleep(self.get_random_delay(2, 5))
                else:
                    time.sleep(self.get_random_delay(1, 2))  # æ²¡æ•°æ®çš„åŸå¸‚ä¼‘æ¯æ—¶é—´çŸ­ä¸€äº›
        
        logger.info(f"ğŸ‰ è½¦168çˆ¬å–å®Œæˆï¼Œæ€»å…±è·å¾— {total_scraped} æ¡æ•°æ®")
        return total_scraped
    
    def _parse_che168_card_enhanced(self, card, city_code):
        """å¢å¼ºç‰ˆè½¦168å¡ç‰‡è§£æ"""
        try:
            # å¤šç§æ–¹å¼æå–æ ‡é¢˜
            title = ''
            title_selectors = ['[carname]', 'h4', '.card-name', '.car-name', '.title']
            for selector in title_selectors:
                elem = card.select_one(selector)
                if elem:
                    title = elem.get('carname') or elem.get_text(strip=True)
                    if title:
                        break
            
            if not title:
                return None
            
            full_text = card.get_text(strip=True)
            
            # æå–ä»·æ ¼
            price = None
            price_patterns = [
                r'([\d.]+)ä¸‡',
                r'Â¥([\d.]+)ä¸‡',
                r'å”®ä»·[ï¼š:]?([\d.]+)ä¸‡'
            ]
            
            for pattern in price_patterns:
                match = re.search(pattern, full_text)
                if match:
                    price = float(match.group(1))
                    break
            
            if not price or price <= 0:
                return None
            
            # è§£æåŸºæœ¬ä¿¡æ¯
            brand, model = self._parse_car_title(title)
            year = self._extract_year(full_text)
            mileage = self._extract_mileage(full_text)
            displacement = self._extract_displacement(full_text)
            transmission = self._extract_transmission(full_text)
            
            # åŸºæœ¬éªŒè¯
            if not year or not mileage:
                return None
            
            # æå–æ›´å¤šå­—æ®µ
            color = self._extract_color(full_text)
            condition_score = self._extract_condition_score(full_text)
            fuel_consumption = self._extract_fuel_consumption(full_text)
            max_speed = self._extract_max_speed(full_text)
            acceleration = self._extract_acceleration(full_text)
            
            # æ„å»ºæ•°æ®å­—å…¸ï¼ŒåªåŒ…å«æœ‰å€¼çš„å­—æ®µ
            data = {
                'å“ç‰Œ': brand,
                'è½¦å‹': model,
                'ä»·æ ¼': price,
                'å¹´ä»½': year,
                'é‡Œç¨‹': mileage,
                'ç‡ƒæ–™ç±»å‹': self._infer_fuel_type(brand, model),
                'å˜é€Ÿå™¨': transmission or 'è‡ªåŠ¨',
                'è½¦è¾†ç±»å‹': self._infer_car_type(model),
                'æ•°æ®æ¥æº': 'è½¦168',
                'æ‰€åœ¨åŸå¸‚': self._city_name_map(city_code),
            }
            
            # åªæ·»åŠ æœ‰å€¼çš„å¯é€‰å­—æ®µ
            if displacement is not None:
                data['æ’é‡'] = displacement
            if color:
                data['é¢œè‰²'] = color
            if condition_score is not None:
                data['è½¦å†µè¯„åˆ†'] = condition_score
            if fuel_consumption is not None:
                data['æ²¹è€—'] = fuel_consumption
            if max_speed is not None:
                data['æœ€é«˜æ—¶é€Ÿ'] = max_speed
            if acceleration is not None:
                data['åŠ é€Ÿæ—¶é—´'] = acceleration
            if year:
                data['è½¦é¾„'] = 2024 - year
            
            data['ä»·æ ¼åŒºé—´'] = self._get_price_range(price)
            data['é‡Œç¨‹åŒºé—´'] = self._get_mileage_range(mileage)
            
            return data
            
        except Exception as e:
            logger.debug(f"è§£æè½¦168å¡ç‰‡å¤±è´¥: {e}")
            return None
    
    
    
    
    
    def _is_valid_data(self, data):
        """éªŒè¯æ•°æ®æœ‰æ•ˆæ€§"""
        required_fields = ['å“ç‰Œ', 'è½¦å‹', 'ä»·æ ¼', 'å¹´ä»½', 'é‡Œç¨‹']
        
        for field in required_fields:
            if not data.get(field):
                return False
        
        # ä»·æ ¼èŒƒå›´æ£€æŸ¥
        if not (0.1 <= data['ä»·æ ¼'] <= 2000):
            return False
        
        # å¹´ä»½èŒƒå›´æ£€æŸ¥
        if not (2000 <= data['å¹´ä»½'] <= 2025):
            return False
        
        # é‡Œç¨‹èŒƒå›´æ£€æŸ¥
        if not (0 <= data['é‡Œç¨‹'] <= 1000000):
            return False
        
        return True
    
    def scrape_all_sources(self, target_count=20000):
        """çˆ¬å–è½¦168æ•°æ®æº"""
        logger.info(f"å¼€å§‹çˆ¬å–è½¦168æ•°æ®ï¼Œç›®æ ‡: {target_count} æ¡æ•°æ®")
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        self.load_checkpoint()
        
        current_count = len(self.data)
        remaining = target_count - current_count
        
        if remaining <= 0:
            logger.info(f"å·²è¾¾åˆ°ç›®æ ‡æ•°æ®é‡: {current_count} æ¡")
            return
        
        logger.info(f"å½“å‰æ•°æ®: {current_count} æ¡ï¼Œè¿˜éœ€: {remaining} æ¡")
        
        try:
            logger.info(f"çˆ¬å–è®¡åˆ’: è½¦168({remaining}æ¡)")
            
            # åªçˆ¬å–è½¦168æ•°æ®
            scraped = self.scrape_che168_enhanced(target_count=remaining)
            logger.info(f"è½¦168å®Œæˆ: {scraped} æ¡")
            self.save_checkpoint()
            
        except KeyboardInterrupt:
            logger.info("ç”¨æˆ·ä¸­æ–­çˆ¬å–...")
            self.save_checkpoint()
            raise
        except Exception as e:
            logger.error(f"çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
            self.save_checkpoint()
            raise
    
    def preprocess_data(self):
        """æ•°æ®é¢„å¤„ç†"""
        logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        
        if not self.data:
            logger.warning("æ²¡æœ‰æ•°æ®éœ€è¦å¤„ç†")
            return
        
        df = pd.DataFrame(self.data)
        initial_count = len(df)
        
        # 1. å»é‡
        df = df.drop_duplicates(subset=['å“ç‰Œ', 'è½¦å‹', 'å¹´ä»½', 'é‡Œç¨‹', 'ä»·æ ¼'], keep='first')
        dedup_count = initial_count - len(df)
        logger.info(f"å»é‡ç§»é™¤: {dedup_count} æ¡")
        
        # 2. æ•°æ®ç±»å‹è½¬æ¢
        numeric_columns = ['ä»·æ ¼', 'å¹´ä»½', 'é‡Œç¨‹', 'æ’é‡']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # 3. ç§»é™¤å¼‚å¸¸å€¼
        df = df.dropna(subset=['å“ç‰Œ', 'è½¦å‹', 'ä»·æ ¼', 'å¹´ä»½', 'é‡Œç¨‹'])
        df = df[(df['ä»·æ ¼'] >= 0.1) & (df['ä»·æ ¼'] <= 2000)]
        df = df[(df['å¹´ä»½'] >= 2000) & (df['å¹´ä»½'] <= 2025)]
        df = df[(df['é‡Œç¨‹'] >= 0) & (df['é‡Œç¨‹'] <= 1000000)]
        
        # 4. å¡«å……ç¼ºå¤±å€¼
        if 'æ’é‡' in df.columns:
            df['æ’é‡'].fillna(0.0, inplace=True)
        
        # 5. æ•°æ®æ ‡å‡†åŒ–
        df['ä»·æ ¼'] = df['ä»·æ ¼'].round(1)
        df['æ’é‡'] = df['æ’é‡'].round(1)
        
        self.data = df.to_dict('records')
        final_count = len(self.data)
        
        logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆ: {initial_count} â†’ {final_count} æ¡")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.print_statistics()
    
    def print_statistics(self):
        """æ‰“å°æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        if not self.data:
            return
        
        df = pd.DataFrame(self.data)
        
        logger.info("=== æ•°æ®ç»Ÿè®¡ä¿¡æ¯ ===")
        logger.info(f"æ€»æ•°æ®é‡: {len(df)} æ¡")
        
        # æ•°æ®æºåˆ†å¸ƒï¼ˆåº”è¯¥å…¨éƒ¨æ¥è‡ªè½¦168ï¼‰
        if 'æ•°æ®æ¥æº' in df.columns:
            source_counts = df['æ•°æ®æ¥æº'].value_counts()
            logger.info("æ•°æ®æºåˆ†å¸ƒ:")
            for source, count in source_counts.items():
                logger.info(f"  {source}: {count} æ¡")
        
        # å“ç‰Œåˆ†å¸ƒ
        brand_counts = df['å“ç‰Œ'].value_counts().head(10)
        logger.info("çƒ­é—¨å“ç‰Œ (Top 10):")
        for brand, count in brand_counts.items():
            logger.info(f"  {brand}: {count} æ¡")
        
        # ä»·æ ¼ç»Ÿè®¡
        logger.info(f"ä»·æ ¼èŒƒå›´: {df['ä»·æ ¼'].min():.1f} - {df['ä»·æ ¼'].max():.1f} ä¸‡å…ƒ")
        logger.info(f"å¹³å‡ä»·æ ¼: {df['ä»·æ ¼'].mean():.1f} ä¸‡å…ƒ")
        
        # å¹´ä»½åˆ†å¸ƒ
        logger.info(f"å¹´ä»½èŒƒå›´: {df['å¹´ä»½'].min()} - {df['å¹´ä»½'].max()}")
        
        # åŸå¸‚åˆ†å¸ƒ
        if 'æ‰€åœ¨åŸå¸‚' in df.columns:
            city_counts = df['æ‰€åœ¨åŸå¸‚'].value_counts().head(10)
            logger.info("åŸå¸‚åˆ†å¸ƒ (Top 10):")
            for city, count in city_counts.items():
                logger.info(f"  {city}: {count} æ¡")
    
    def save_data(self, filename=None):
        """ä¿å­˜æ•°æ®åˆ°CSV"""
        if not self.data:
            logger.warning("æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return False
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"car_data_{timestamp}.csv"
        
        try:
            df = pd.DataFrame(self.data)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            file_size = os.path.getsize(filename) / 1024 / 1024
            logger.info(f"æ•°æ®å·²ä¿å­˜: {filename}")
            logger.info(f"æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
            logger.info(f"æ•°æ®æ¡æ•°: {len(df)} æ¡")
            
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    scraper = CarDataScraper()
    
    print("ğŸš— æ±½è½¦æ•°æ®çˆ¬è™«ç³»ç»Ÿ - è½¦168ä¸“ç‰ˆ")
    print("ç›®æ ‡: ä»è½¦168çˆ¬å–20000+æ¡çœŸå®æ±½è½¦æ•°æ®")
    print("=" * 60)
    
    try:
        target_count = 20000
        print(f"å¼€å§‹çˆ¬å– {target_count} æ¡æ±½è½¦æ•°æ®...")
        
        start_time = time.time()
        
        # å¼€å§‹çˆ¬å–
        scraper.scrape_all_sources(target_count=target_count)
        
        # æ•°æ®é¢„å¤„ç† - æš‚æ—¶éšè—
        # scraper.preprocess_data()
        
        # ä¿å­˜æ•°æ®
        success = scraper.save_data()
        
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        print("\n" + "=" * 60)
        print("ğŸ‰ çˆ¬å–å®Œæˆ!")
        print(f"â° æ€»è€—æ—¶: {elapsed_time/60:.1f} åˆ†é’Ÿ")
        print(f"ğŸ“Š æœ€ç»ˆæ•°æ®é‡: {len(scraper.data)} æ¡")
        
        if success:
            print("âœ… æ•°æ®çˆ¬å–å’Œä¿å­˜æˆåŠŸ!")
        else:
            print("âŒ æ•°æ®ä¿å­˜å¤±è´¥!")
        
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­çˆ¬å–...")
        if scraper.data:
            print(f"å·²çˆ¬å– {len(scraper.data)} æ¡æ•°æ®")
            save_choice = input("æ˜¯å¦ä¿å­˜å·²çˆ¬å–çš„æ•°æ®? (y/n): ").lower()
            if save_choice == 'y':
                scraper.save_data()
    
    except Exception as e:
        logger.error(f"çˆ¬å–è¿‡ç¨‹å‡ºç°é”™è¯¯: {str(e)}")
        if scraper.data:
            logger.info(f"å°è¯•ä¿å­˜å·²çˆ¬å–çš„ {len(scraper.data)} æ¡æ•°æ®...")
            scraper.save_data()
    
    finally:
        scraper.close_driver()

if __name__ == "__main__":
    main()